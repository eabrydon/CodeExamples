# CodeExamples
This repository contains code from some projects I've worked on

The files labeled AXA_Kaggle are from the Kaggle competition I did with Matt Shadish and Monica Meyer for the final project in our Advanced Machine Learning class. A little background on the data for this competition: We were provided with several hundred folders, each containing several hundred CSVs. Each CSV was a "trip" made by a driver, in the form of x,y coordinates given once per second for the duration of the trip. Each folder contained mostly trips by one driver, but a smaller percentage of trips made by other drivers. Kaggle did not label any of the trips, and the goal of the competition was to successfully classify the trips as belonging to the given driver for that folder or not, thus successfully establishing a "driver fingerprint". More information about the competition can be found at https://www.kaggle.com/c/axa-driver-telematics-analysis.

The R file label FE is the feature extraction. It goes through the folders of trips and flattens each trip into a line in a CSV, so each folder has a corresponding CSV. From the x,y coordinates, we made some simple features, like number of turns and average speed, and some more complex features, like centripetal acceleration while turning. We have quite a few features because we weren't sure which ones would be useful at first. Matt Shadish wrote most of this code, but I made a significant contribution and it provides context for the next file, so I included it.

The py file labeled ML is the machine learning we did. The competition did not give labels for the trips, so to get around that, for each folder we wanted to look at we pulled in trips from other folders to use as negative examples. This gave us a way to train a supervised model to make classifications. However, we had to be careful, because we were inevitably labeling some examples that were really negative as positive in the process. To make sure we didn't overfit, we used a random forest where the trees had a maximum depth of 3, meaning there could only be 3 divisions of the data to increase purity. In this way, we stopped the random forest from perfectly fitting each point, so when we put the original points back into the model, it would reclassify some positively labeled examples as negatives. To further increase our accuracy, we combined the results from our random forest with an ensemble of bagged logistic regressions. We chose logistic regressions because they could train and test quickly, and ensembles of different types of learners often do better than any of their individual parts.